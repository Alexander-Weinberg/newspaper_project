{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Unstructured Text into Structured Data \n",
    "Online supplementary material to \"The Evolving U.S. Occupational Structure: A Textual Analysis.\" \n",
    "\n",
    "* [Most recent version of the paper](\n",
    "http://ssc.wisc.edu/~eatalay/skills.pdf)\n",
    "\n",
    "* [Project data library](http://ssc.wisc.edu/~eatalay/occupation_data.html) \n",
    "\n",
    "* [GitHub repository](https://github.com/phaiptt125/newspaper_project)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython notebook demonstrates how we finally transform unstructured newspaper text into structured data (spreadsheet). In the previous steps, we:\n",
    "\n",
    "* Retrieve document metadata, remove markup from the newspaper text, and to perform an initial spell-check of the text (see [here](https://github.com/phaiptt125/newspaper_project/blob/master/data_cleaning/LDA.ipynb)). \n",
    "* Exclude non-job ad pages (see [here](https://github.com/phaiptt125/newspaper_project/blob/master/data_cleaning/initial_cleaning.ipynb)).\n",
    "\n",
    "The main components of this step are to identify the job title, discern the boundaries between job ads and transform relevant information into structured data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Due to copyright restrictions, we are not authorized to publish a large body of newspaper text. </b>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of auxiliary files (see project data library or GitHub repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *ExtractLDAresult.py* : This python code extracts machine-readable results from the LDA estimation.\n",
    "* *compute_spelling.py* : This python code computes ratio of correctly-spelled words and records all correctly-spelled words.\n",
    "***\n",
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sys.path.append('./auxiliary files')\n",
    "\n",
    "from compute_spelling import *\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from ExtractLDAresult import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
